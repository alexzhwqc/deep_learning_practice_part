batch_size    128
code_file    ptb-lm.py
data    data
debug    False
dp_keep_prob    0.9
emb_size    200
evaluate    False
hidden_size    256
initial_lr    0.001
model    TRANSFORMER
num_epochs    40
num_layers    5
optimizer    ADAM
save_best    False
save_dir    TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.001_batch_size=128_seq_len=60_hidden_size=256_num_layers=5_dp_keep_prob=.9_1
seed    1111
seq_len    60
