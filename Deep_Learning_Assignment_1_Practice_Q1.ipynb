{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.load(\"mnist.npz\")\n",
    "X_train, Y_train = data[\"x_train\"], data[\"y_train\"]\n",
    "X_test, Y_test = data[\"x_test\"], data[\"y_test\"]\n",
    "X_train = X_train.reshape(X_train.shape[0], -1) / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], -1) / 255\n",
    "\n",
    "# Split train and validation data as 50000:10000\n",
    "train_size = 50000\n",
    "X_train_ = X_train[:train_size]\n",
    "X_valid_ = X_train[train_size:]\n",
    "Y_train_ = Y_train[:train_size]\n",
    "Y_valid_ = Y_train[train_size:]\n",
    "\n",
    "X_train = X_train_\n",
    "X_valid = X_valid_\n",
    "Y_train = Y_train_\n",
    "Y_valid = Y_valid_\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "def debug(*args, **kwargs):\n",
    "    global DEBUG\n",
    "    if DEBUG:\n",
    "        print(*args, **kwargs)\n",
    "\n",
    "# Dimensionality of MNIST input\n",
    "N_INPUT = 28*28\n",
    "# Total classes number\n",
    "N_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 hidden layers MLP with numpy\n",
    "class NN(object):\n",
    "    def __init__(self, hidden_dims = (450, 400),  # (number of paras between 0.5M and 1M)\n",
    "                 n_hidden = 2, mode = \"train\",\n",
    "                 datapath = None, model_path = None):\n",
    "        \n",
    "        #bias list for saving all bias\n",
    "        self.bias = []\n",
    "        # Weights list for saving all weights\n",
    "        self.weights = []\n",
    "                \n",
    "        # Init weights and bias of first layer\n",
    "        self.bias.append(np.zeros((hidden_dims[0])))\n",
    "        self.weights.append(np.empty((hidden_dims[0], N_INPUT)))\n",
    "                \n",
    "        # Init weights and bias of hidden layers\n",
    "        for i in range(n_hidden - 1):\n",
    "            self.bias.append(np.zeros((hidden_dims[i+1])))\n",
    "            self.weights.append(np.empty((hidden_dims[i+1], hidden_dims[i])))\n",
    "                    \n",
    "        # Init weights and bias of output layer\n",
    "        self.bias.append(np.zeros((N_CLASSES)))\n",
    "        self.weights.append(np.empty((N_CLASSES, hidden_dims[-1])))\n",
    "        \n",
    "    def initialize_weights(self, method=\"glorot\"):\n",
    "        # Init weights in order of layers\n",
    "        for idx, weight in enumerate(self.weights):\n",
    "            if method == \"zero\":\n",
    "                self.weights[idx] = np.zeros(shape = weight.shape)\n",
    "            elif method == \"normal\":\n",
    "                self.weights[idx] = np.random.normal(loc = 0, scale = 1, size = weight.shape)\n",
    "            elif method == \"glorot\":\n",
    "                d = np.sqrt(6 / (weight.shape[0] + weight.shape[1]))\n",
    "                self.weights[idx] = np.random.uniform(low = -d, high = d, size = weight.shape)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.cache = [input]\n",
    "        for weight, bias in zip(self.weights, self.bias):\n",
    "            self.cache.append(self.activation(np.dot(weight, self.cache[-1]) + bias))\n",
    "        last_cache = self.cache.pop()\n",
    "        return self.softmax(last_cache)\n",
    "\n",
    "    def activation(self, input):\n",
    "        return np.maximum(0, input)\n",
    "\n",
    "    def loss(self, prediction, label):\n",
    "        return -(np.log(self.softmax(prediction)[label]))\n",
    "\n",
    "    def softmax(self, input):\n",
    "        max_input = np.max(input)\n",
    "        return np.exp(input - max_input) / np.sum(np.exp(input - max_input))\n",
    "\n",
    "    def backward(self, output, label):\n",
    "        tmp_list = []\n",
    "        for idx, o in enumerate(output):\n",
    "            if idx == label:\n",
    "                tmp_list.append(o - 1)\n",
    "            else:\n",
    "                tmp_list.append(o)\n",
    "        pre_activation_grads = np.asarray(tmp_list).reshape(-1, 1)\n",
    "        \n",
    "        # Bias gradients\n",
    "        self.bias_grads = []\n",
    "        # Weight gradients \n",
    "        self.weight_grads = []\n",
    "                \n",
    "        for idx, (weight, _) in enumerate(zip(reversed(self.weights), reversed(self.bias))):\n",
    "            layer_passed = np.asarray(list(reversed(self.cache))[idx])\n",
    "            self.weight_grads.insert(0, np.dot(pre_activation_grads,layer_passed.reshape(-1, 1).T))\n",
    "            self.bias_grads.insert(0, pre_activation_grads.reshape(-1))\n",
    "            layer_passed_grads = np.dot(weight.T , pre_activation_grads)\n",
    "            \n",
    "            tmp_list = []\n",
    "            for cached in list(reversed(self.cache))[idx]:\n",
    "                if cached > 0:\n",
    "                    tmp_list.append(1)\n",
    "                else:\n",
    "                    tmp_list.append(0)\n",
    "            pre_activation_grads = layer_passed_grads * np.asarray(tmp_list).reshape(-1, 1)\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        for idx, (weight_grad, bias_grad) in enumerate(zip(self.weight_grads, self.bias_grads)):\n",
    "            self.bias[idx] = self.bias[idx] - learning_rate * bias_grad\n",
    "            self.weights[idx] = self.weights[idx] - learning_rate * weight_grad\n",
    "\n",
    "    def train(self, inputs, labels, N_epochs = 1, lr = 0.001, log = True):\n",
    "        losses = []\n",
    "        for epoch in range(N_epochs):\n",
    "            loss = []\n",
    "            for idx, (input, label) in enumerate(zip(inputs, labels), 1):\n",
    "                result = self.forward(input)\n",
    "                loss.append(nn.loss(result, label))\n",
    "                \n",
    "                    \n",
    "                self.backward(result, label)\n",
    "                self.update(lr)\n",
    "            losses.append(np.mean(loss))\n",
    "        return losses\n",
    "    \n",
    "    def test(self, inputs, labels):\n",
    "        loss, acc = zip(*[(self.loss(self.forward(input), label), np.argmax(self.forward(input)) == label)\n",
    "                          for input,label in zip(inputs, labels)])\n",
    "        return np.mean(loss), np.mean(acc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NN()\n",
    "nn.initialize_weights(\"zero\")\n",
    "zero_loss = nn.train(X_train, Y_train, N_epochs = 10, lr = 1e-3)\n",
    "\n",
    "nn = NN()\n",
    "nn.initialize_weights(\"normal\")\n",
    "normal_loss = nn.train(X_train, Y_train, N_epochs = 10, lr = 1e-3)\n",
    "\n",
    "nn = NN()\n",
    "nn.initialize_weights(\"glorot\")\n",
    "glorot_loss = nn.train(X_train, Y_train, N_epochs = 10, lr = 1e-3)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(range(1, 11), zero_loss, label=\"zero\", color='b')\n",
    "plt.plot(range(1, 11), normal_loss, label=\"normal\", color='c')\n",
    "plt.plot(range(1, 11), glorot_loss, label=\"glorot\", color='r')\n",
    "plt.legend(title = \"Initialization\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "# Grid search for best hyperparameters\n",
    "def grid_search():\n",
    "    best_h1 = -1\n",
    "    best_h2 = -1\n",
    "    best_lr = -1\n",
    "    best_nn = None\n",
    "    best_acc = -1\n",
    "        \n",
    "    for h1 in [450, 350, 150]:\n",
    "        for h2 in [400, 200, 100]:\n",
    "            for lr_item in [0.1, 0.01, 0.001]:\n",
    "                nn = NN(hidden_dims = (h1, h2))\n",
    "                nn.initialize_weights(\"glorot\")\n",
    "                nn.train(X_train, Y_train, N_epochs = 10, lr = lr_item, log = False)\n",
    "                _, acc = nn.test(X_valid, Y_valid)\n",
    "                print(\"h1:{0}, h2:{1}, lr:{2}, acc:{3}\".format(h1, h2, lr_item, acc))\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_nn = nn\n",
    "                    best_h1 = h1\n",
    "                    best_h2 = h2\n",
    "                    best_lr = lr_item\n",
    "                    \n",
    "    return best_nn\n",
    "\n",
    "_, acc = grid_search().test(X_test, Y_test)\n",
    "print(\"Acc on the test set: {:.3%}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
